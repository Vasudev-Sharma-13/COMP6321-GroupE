{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import ceil\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils, datasets\n",
    "import os\n",
    "​\n",
    "import matplotlib.pyplot as plt\n",
    "​\n",
    "import torch.utils.data as td\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from matplotlib import image\n",
    "from matplotlib import pyplot\n",
    "from collections import defaultdict\n",
    "​\n",
    "import numpy as np\n",
    "# from scipy.signal import savgol_filter\n",
    "import time\n",
    "from torchvision import models\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay,classification_report\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "import pickle\n",
    "​\n",
    "import json\n",
    "from PIL import Image as PilImage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = new_path = \"/kaggle/input/comp6321-project-datasets/Dataset 2/Dataset 2/Prostate Cancer\"\n",
    "subDirectories = os.listdir(new_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Device to GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading Labels of Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_label(subDirectories):\n",
    "    print(subDirectories)\n",
    "    enu = range(len(subDirectories))\n",
    "    labels_map={}\n",
    "    for enu,subDirectory in enumerate(subDirectories):\n",
    "        labels_map.update({enu:subDirectory})\n",
    "    print(labels_map)\n",
    "    print(\"The number of labels in the dataset = %s\"% len(labels_map))\n",
    "    print(\"The \\'%s\\' device is being used to process the dataset\"%device)\n",
    "    return labels_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_map = read_label(subDirectories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Display Dataset Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def displayImages(t_dataset,imageFlag):\n",
    "    figure = plt.figure(figsize=(8, 8))\n",
    "    cols, rows = 3, 3\n",
    "    if imageFlag==0:\n",
    "      figure.suptitle(\"Images before preprocessing\")\n",
    "    else:\n",
    "     figure.suptitle(\"Images after preprocessing\")   \n",
    "    for i in range(1, cols * rows + 1):\n",
    "      sample_idx = torch.randint(len(t_dataset), size=(1,)).item()\n",
    "      img, label = t_dataset[sample_idx]\n",
    "      img=np.array(img).transpose((1,2,0))\n",
    "      figure.add_subplot(rows, cols, i)\n",
    "      plt.title(labels_map[label])\n",
    "      plt.axis(\"off\")\n",
    "      plt.imshow(img.squeeze())\n",
    "    print(\"\\n\\n\\n################################################\\n\\n\\n\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_loader(data_dir_input,batch_sizeGiven,input_size,flag=0,test_split=1):\n",
    "# Define dataset directory and transforms\n",
    "  data_dir = data_dir_input #\n",
    "  \n",
    "  data_transform = transforms.Compose([\n",
    "    transforms.Resize(input_size),\n",
    "    transforms.RandomRotation(20),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(0.1,0.1,0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.554, 0.450, 0.343],[0.231, 0.241, 0.241]),\n",
    "  ])\n",
    "\n",
    "\n",
    "  if flag==1:\n",
    "    test_temp_dataset = datasets.ImageFolder(data_dir,transform=transforms.Compose([\n",
    "    transforms.Resize(input_size),\n",
    "    transforms.ToTensor(),   \n",
    "  ]))\n",
    "    displayImages(test_temp_dataset,0)\n",
    "    test_temp_dataset = datasets.ImageFolder(data_dir,transform=data_transform)\n",
    "    displayImages(test_temp_dataset,1)\n",
    "\n",
    "    data = datasets.ImageFolder(root=data_dir, transform=data_transform)\n",
    "    # Define test dataset\n",
    "    dataset_size = len(data)\n",
    "\n",
    "\n",
    "    # Create a dictionary to store the number of images per class\n",
    "    num_images_per_class = defaultdict(int)\n",
    "\n",
    "    # Print the number of images per class\n",
    "    for label, num_images in num_images_per_class.items():\n",
    "        print(f\"Class {label}: {num_images} images\")\n",
    "  \n",
    "\n",
    "    # Define the number of classes and generate a list of colors\n",
    "    num_classes = len(num_images_per_class)\n",
    "    colors = plt.cm.Set1(np.linspace(0, 1, num_classes))\n",
    "    # Plot the number of images per class as a bar plot\n",
    "    plt.bar(num_images_per_class.keys(), num_images_per_class.values(), color=colors)\n",
    "    \n",
    "    # Add axis labels and title\n",
    "    plt.xlabel(\"Class\")\n",
    "    plt.ylabel(\"Number of images\")\n",
    "    plt.title(\"Number of images per class in the dataset\")\n",
    "\n",
    "    # Create a legend\n",
    "    labels = [f\"Class {label}\" for label in num_images_per_class.keys()]\n",
    "    plt.legend([plt.bar(0, 0, color=colors[i])[0] for i in range(len(num_images_per_class))], labels,loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    plt.rcParams[\"figure.figsize\"] = (40,15)\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "    \n",
    "    return  \n",
    "\n",
    "  data = datasets.ImageFolder(root=data_dir, transform=data_transform)\n",
    "  test_loader = DataLoader(data, batch_size=batch_sizeGiven, shuffle=True, drop_last=False, num_workers=0,pin_memory=True)\n",
    "\n",
    "  return test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Images before and after Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=32\n",
    "inputDimension=(224,224)\n",
    "data_loader(path,batch_size,inputDimension,flag=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Trained Model (Without Transfer Learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = torch.load('/kaggle/input/models/COMP6321_ResNet_Task1_CancerDataset_Model_Final_HyperParamaterTuning8.pth')\n",
    "model1.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T-SNE Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get embeddings for the test data\n",
    "def tSNE_plot(temporaryModel):\n",
    "    embeddings = []\n",
    "    labels = []\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            images, targets = data[0].to(device), data[1].to(device)\n",
    "            outputs = temporaryModel(images)\n",
    "            embeddings.append(outputs.cpu().numpy())\n",
    "            labels.append(targets.cpu().numpy())\n",
    "    embeddings = np.concatenate(embeddings)\n",
    "    labels = np.concatenate(labels)\n",
    "\n",
    "      # Apply t-SNE for visualization\n",
    "    tsne = TSNE(n_components=2, perplexity=30, random_state=0)\n",
    "    embeddings_tsne = tsne.fit_transform(embeddings)\n",
    "\n",
    "      # Plot t-SNE embeddings\n",
    "    plt.figure(figsize=(30,15))\n",
    "\n",
    "    for i in range(len(labels_map)):\n",
    "        plt.scatter(embeddings_tsne[labels==i,0], embeddings_tsne[labels==i,1], label=f'Class {i}')\n",
    "    plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    plt.title('t-SNE Embeddings for Training Data')\n",
    "    plt.xlabel(\"t-SNE component 1\")\n",
    "    plt.ylabel(\"t-SNE component 2\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot T_SNE before Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tSNE_plot(model1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove Last Fully Connected (FC) Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newmodel = torch.nn.Sequential(*(list(model1.children())[:-1]))\n",
    "newmodel.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Custom Dataset Using Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define the transformations\n",
    "TempTransformer = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomRotation(20),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(0.1,0.1,0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.554, 0.450, 0.343],[0.231, 0.241, 0.241]),\n",
    "])\n",
    "\n",
    "# Function to extract features using the model\n",
    "def extract_features(image_path, model, device):\n",
    "    image = Image.open(image_path)\n",
    "    with torch.no_grad():\n",
    "        features = model(TempTransformer(image).unsqueeze(0).to(device))\n",
    "    image.close()\n",
    "    return features.cpu().numpy().flatten()\n",
    "\n",
    "# Define the images\n",
    "images = []\n",
    "enu = range(len(subDirectories))\n",
    "labels_map={}\n",
    "for enu,subDirectory in enumerate(subDirectories):\n",
    "    labels_map.update({enu:subDirectory})\n",
    "for j in subDirectories:\n",
    "    for i in os.listdir(new_path+'/'+j):\n",
    "        images.append(new_path+'/'+j+'/'+i)\n",
    "\n",
    "# Load the model and set to evaluation mode\n",
    "model1 = torch.nn.Sequential(*(list(model1.children())[:-1]))\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model1 = model1.to(device).eval()\n",
    "\n",
    "# Extract features for each image\n",
    "all_features = []\n",
    "for i, image_path in enumerate(tqdm(images)):\n",
    "    features = extract_features(image_path, model1, device)\n",
    "    all_features.append(features)\n",
    "\n",
    "# Convert the features to a DataFrame\n",
    "columns = [f\"feature_{i}\" for i in range(len(all_features[0]))]\n",
    "df = pd.DataFrame(all_features, columns=columns)\n",
    "\n",
    "# Save the features to a CSV file\n",
    "df.to_csv(\"extracted_features.csv\", index=False)\n",
    "print(\"Features saved to 'extracted_features.csv'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracted 512 Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
