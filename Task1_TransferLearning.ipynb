{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fa9d9f9",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c73931",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import ceil\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils, datasets\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch.utils.data as td\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from matplotlib import image\n",
    "from matplotlib import pyplot\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "from scipy.signal import savgol_filter\n",
    "import time\n",
    "from torchvision import models\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay,classification_report\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "import pickle\n",
    "from thop import profile\n",
    "\n",
    "\n",
    "import json\n",
    "from PIL import Image as PilImage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1dcf1bc",
   "metadata": {},
   "source": [
    "## Setting Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ee5bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = new_path = \"C:/Users/vshar/Downloads/Dataset 1/Dataset 1/Colorectal Cancer\"\n",
    "subDirectories = os.listdir(new_path)\n",
    "saveFilePath=\"C:/Users/vshar/Documents/hyperparameters.pkl\"\n",
    "saveModelPath=\"C:/Users/vshar/Documents\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd359d07",
   "metadata": {},
   "source": [
    "## Set device to GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28796a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0bb73c",
   "metadata": {},
   "source": [
    "## Reading the labels of the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d3ce58",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(subDirectories)\n",
    "\n",
    "enu = range(len(subDirectories))\n",
    "labels_map={}\n",
    "for enu,subDirectory in enumerate(subDirectories):\n",
    "    labels_map.update({enu:subDirectory})\n",
    "print(labels_map)\n",
    "print(\"The number of labels in the dataset = %s\"% len(labels_map))\n",
    "print(\"The \\'%s\\' device is being used to process the dataset\"%device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7090af8b",
   "metadata": {},
   "source": [
    "## Display Dataset Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57e8de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def displayImages(t_dataset,imageFlag):\n",
    "    figure = plt.figure(figsize=(8, 8))\n",
    "    cols, rows = 3, 3\n",
    "    if imageFlag==0:\n",
    "      figure.suptitle(\"Images before preprocessing\")\n",
    "    else:\n",
    "     figure.suptitle(\"Images after preprocessing\")   \n",
    "    for i in range(1, cols * rows + 1):\n",
    "      sample_idx = torch.randint(len(t_dataset), size=(1,)).item()\n",
    "      img, label = t_dataset[sample_idx]\n",
    "      img=np.array(img).transpose((1,2,0))\n",
    "      figure.add_subplot(rows, cols, i)\n",
    "      plt.title(labels_map[label])\n",
    "      plt.axis(\"off\")\n",
    "      plt.imshow(img.squeeze())\n",
    "    print(\"\\n\\n\\n################################################\\n\\n\\n\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48232f78",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cce930f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_loader(data_dir_input,batch_sizeGiven,input_size,flag=0,test_split=0.1,val_split=0.1):\n",
    "# Define dataset directory and transforms\n",
    "  data_dir = data_dir_input #\n",
    "  \n",
    "  data_transform = transforms.Compose([\n",
    "    transforms.Resize(input_size),\n",
    "    transforms.RandomRotation(20),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(0.1,0.1,0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.554, 0.450, 0.343],[0.231, 0.241, 0.241]),\n",
    "  ])\n",
    "\n",
    "\n",
    "  if flag==1:\n",
    "    test_temp_dataset = datasets.ImageFolder(data_dir,transform=transforms.Compose([\n",
    "    transforms.Resize(input_size),\n",
    "    transforms.ToTensor(),   \n",
    "  ]))\n",
    "    displayImages(test_temp_dataset,0)\n",
    "    test_temp_dataset = datasets.ImageFolder(data_dir,transform=data_transform)\n",
    "    displayImages(test_temp_dataset,1)\n",
    "\n",
    "    data = datasets.ImageFolder(root=data_dir, transform=data_transform)\n",
    "    # Define train, validation, and test dataset\n",
    "    dataset_size = len(data)\n",
    "    test_size = int(test_split * dataset_size)\n",
    "    val_size = int(val_split * dataset_size)\n",
    "    train_size = dataset_size - (test_size + val_size)\n",
    "    train_dataset, test_dataset, val_dataset = td.random_split(data,[train_size, test_size, val_size])  \n",
    "    print(\"Train Datset Size After Split\",len(train_dataset))\n",
    "    print(\"Test Datset Size After Split\",len(test_dataset))\n",
    "    print(\"Validation Datset Size After Split\",len(val_dataset))\n",
    "\n",
    "\n",
    "    # Create a dictionary to store the number of images per class\n",
    "    num_images_per_class = defaultdict(int)\n",
    "\n",
    "    # Iterate over the training dataset and count the number of images in each class\n",
    "    for image, label in train_dataset:\n",
    "        num_images_per_class[label] += 1\n",
    "\n",
    "    # Print the number of images per class\n",
    "    for label, num_images in num_images_per_class.items():\n",
    "        print(f\"Class {label}: {num_images} images\")\n",
    "  \n",
    "\n",
    "    # Define the number of classes and generate a list of colors\n",
    "    num_classes = len(num_images_per_class)\n",
    "    colors = plt.cm.Set1(np.linspace(0, 1, num_classes))\n",
    "\n",
    "    # Plot the number of images per class as a bar plot\n",
    "    plt.bar(num_images_per_class.keys(), num_images_per_class.values(), color=colors)\n",
    "    #plt.pie(num_images_per_class.values(), labels=num_images_per_class.keys(), colors=colors, autopct='%1.1f%%')\n",
    "    # Add axis labels and title\n",
    "    plt.xlabel(\"Class\")\n",
    "    plt.ylabel(\"Number of images\")\n",
    "    plt.title(\"Number of images per class in the dataset\")\n",
    "\n",
    "    # Create a legend\n",
    "    labels = [f\"Class {label}\" for label in num_images_per_class.keys()]\n",
    "    plt.legend([plt.bar(0, 0, color=colors[i])[0] for i in range(len(num_images_per_class))], labels,loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    plt.rcParams[\"figure.figsize\"] = (40,15)\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "    \n",
    "    return  \n",
    "\n",
    "  data = datasets.ImageFolder(root=data_dir, transform=data_transform)\n",
    "  # Define train, validation, and test dataset\n",
    "  dataset_size = len(data)\n",
    "  test_size = int(test_split * dataset_size)\n",
    "  val_size = int(val_split * dataset_size)\n",
    "  train_size = dataset_size - (test_size + val_size)\n",
    "  train_dataset, test_dataset, val_dataset = td.random_split(data,[train_size, test_size, val_size])  \n",
    " \n",
    "  train_loader = DataLoader(train_dataset, batch_size=batch_sizeGiven, shuffle=True, drop_last=False, num_workers=0,pin_memory=True)\n",
    "  test_loader = DataLoader(test_dataset, batch_size=batch_sizeGiven, shuffle=True, drop_last=False, num_workers=0,pin_memory=True) #shuffle \n",
    "  val_loader = DataLoader(val_dataset, batch_size=batch_sizeGiven, shuffle=True, drop_last=False, num_workers=0,pin_memory=True) # shuffle working\n",
    "\n",
    "\n",
    "  return train_loader,test_loader,val_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7de813d",
   "metadata": {},
   "source": [
    "## Dataset images before and after Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096a4713",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=32\n",
    "inputDimension=(224,224)\n",
    "data_loader(path,batch_size,inputDimension,flag=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bcec193",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61b9d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(num_epochsGiven, model, train_loader, criterion, optimizer,val_loader):\n",
    "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "  print(\"Device: {}\".format(device))\n",
    "  model.to(device)\n",
    "  Accuracy=[]\n",
    "  validAccuracy=[]\n",
    "  stepAccuracyTotal=[]\n",
    "  stepAccuracyValidationTotal=[]\n",
    "  stepLossTotal=[]\n",
    "  Loss = []\n",
    "  num_epochs = num_epochsGiven\n",
    "  total_steps = len(train_loader)\n",
    "  t1 = time.time()\n",
    "  total,correct,loss=0,0,0\n",
    "  for epoch in range(num_epochs):\n",
    "      stepAcc=[]\n",
    "      stepLoss=[]    \n",
    "      for i, data in enumerate(train_loader):\n",
    "          images, labels = data[0].to(device), data[1].to(device)\n",
    "          # Forward pass\n",
    "          outputs = model(images)\n",
    "          loss = criterion(outputs, labels)\n",
    "          # Backprop and optimisation\n",
    "          optimizer.zero_grad()\n",
    "          loss.backward()\n",
    "          optimizer.step()\n",
    "          # Train accuracy\n",
    "          total = labels.size(0)\n",
    "          _,predicted = torch.max(outputs.data, 1)\n",
    "          correct = (predicted == labels).sum().item()\n",
    "          stepAcc.append((correct / total) * 100)\n",
    "          stepLoss.append(loss.item())\n",
    "          if (i + 1) % 30 == 0:\n",
    "              stepAccuracyTotal.append((correct / total) * 100)\n",
    "              stepLossTotal.append(loss.item())\n",
    "              correct_v = 0\n",
    "              total_v = 0\n",
    "              for dataVal in val_loader:\n",
    "                  images_v, labels_v = dataVal[0].to(device), dataVal[1].to(device)\n",
    "                  outputs = model(images_v)\n",
    "                  _, predicted = torch.max(outputs.data, 1)\n",
    "                  correct_v += (predicted == labels_v).sum().item()\n",
    "                  total_v += labels_v.size(0) \n",
    "              stepAccuracyValidationTotal.append((correct_v / total_v) * 100)\n",
    "              print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Accuracy: {:.2f}%,Validation Accuracy: {:.2f}%'.format(epoch + 1, num_epochs, i + 1, total_steps, loss.item(),(correct / total) * 100,(correct_v / total_v) * 100))     \n",
    "      correct_v = 0\n",
    "      total_v = 0\n",
    "      for dataVal in val_loader:\n",
    "          images_v, labels_v = dataVal[0].to(device), dataVal[1].to(device)\n",
    "          outputs = model(images_v)\n",
    "          _, predicted = torch.max(outputs.data, 1)\n",
    "          correct_v += (predicted == labels_v).sum().item()\n",
    "          total_v += labels_v.size(0)        \n",
    "      Accuracy.append(sum(stepAcc)/len(stepAcc))\n",
    "      validAccuracy.append((correct_v / total_v) * 100)\n",
    "      Loss.append(sum(stepLoss)/len(stepLoss))\n",
    "  endTime =  time.time()\n",
    "  print(\"######## Training Finished in {} seconds ###########\".format(endTime-t1))\n",
    "  print(\"######## Training timer per epoch is {} seconds ###########\".format((endTime-t1)/num_epochsGiven))\n",
    "  return Loss,Accuracy,model,validAccuracy,stepAccuracyTotal,stepAccuracyValidationTotal,stepLossTotal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9229552",
   "metadata": {},
   "source": [
    "## Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb35ee5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, test_loader):\n",
    "  model.eval() \n",
    "  y_truth=[]\n",
    "  y_predicted=[]\n",
    "  cm=[]\n",
    "  G_correct=0\n",
    "  G_total=0\n",
    "  \n",
    "  with torch.no_grad(): \n",
    "      correct = 0\n",
    "      total = 0\n",
    "      for data in test_loader:\n",
    "          images, labels = data[0].to(device), data[1].to(device)\n",
    "          outputs = model(images)\n",
    "          _, predicted = torch.max(outputs.data, 1)\n",
    "          total += labels.size(0)\n",
    "          correct += (predicted == labels).sum().item()\n",
    "          y_truth+=labels.cpu().numpy().tolist()\n",
    "          y_predicted+=predicted.cpu().numpy().tolist()\n",
    "      print('Test Accuracy of the model on the {} test images: {} %'.format(total, (correct / total) * 100))\n",
    "      cm= confusion_matrix(y_truth,y_predicted)\n",
    "      G_total=total\n",
    "      G_correct=correct\n",
    "  print(classification_report(y_truth,y_predicted))\n",
    "  return cm,((G_correct / G_total) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099a87ae",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfad8ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS=[]\n",
    "hyper_parameters = []\n",
    "train_acc_hyper_paramaters=[]\n",
    "train_acc_valid_hyper_paramaters=[]\n",
    "train_loss_hyper_paramaters=[]\n",
    "\n",
    "train_acc_hyper_paramaters_step=[]\n",
    "train_acc_valid_hyper_paramaters_step=[]\n",
    "train_loss_hyper_paramaters_step=[]\n",
    "test_loss=[]\n",
    "\n",
    "#different batch sizes\n",
    "batch_sizes=[32]\n",
    "#different learning rates\n",
    "learning_rates= [0.0001]\n",
    "#Setting the number of epochs\n",
    "epochs=10\n",
    "\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "   for batch_size in batch_sizes:\n",
    "      #Model ResNet   \n",
    "      model3 = models.resnet18(weights=\"IMAGENET1K_V1\")\n",
    "      model3.fc = nn.Linear(512,len(labels_map))# fine tuning the last layer\n",
    "      stringCrit=\"Cross Entropy Loss\"\n",
    "      criterion=nn.CrossEntropyLoss()\n",
    "      #Optimizer Function \n",
    "      stringOPTM=\"Adam Optimizer\"\n",
    "      optimizer = torch.optim.Adam(model3.parameters(), lr=learning_rate)\n",
    "\n",
    "      hyper_parameters.append([\"Learning Rate: \"+str(learning_rate),\" Batch Size: \"+str(batch_size),stringCrit,stringOPTM])\n",
    "      print(\"Learning Rate: \"+str(learning_rate)+\", Batch Size: \"+str(batch_size),\", Loss function: \",stringCrit,\", Optimizer: \",stringOPTM)\n",
    "      \n",
    "      train_loader, test_loader, val_loader = data_loader(path,batch_size, inputDimension)\n",
    "      tempLoss,tempAccuracy,tempModel,tempValidAccuracy,tempStepAccuracy,tempStepValidation,tempStepLoss = train(epochs,model3,train_loader,criterion,optimizer,val_loader)\n",
    "\n",
    "      \n",
    "      train_loss_hyper_paramaters.append(tempLoss)\n",
    "      train_acc_hyper_paramaters.append(tempAccuracy)\n",
    "      train_acc_valid_hyper_paramaters.append(tempValidAccuracy)\n",
    "\n",
    "      train_acc_valid_hyper_paramaters_step.append(tempStepValidation)      \n",
    "      train_acc_hyper_paramaters_step.append(tempStepAccuracy)\n",
    "      train_loss_hyper_paramaters_step.append(tempStepLoss)\n",
    "\n",
    "      cmReturned,temp_test_loss = test(model3,\"cuda\",test_loader)\n",
    "      test_loss.append([temp_test_loss,learning_rate])\n",
    "      if cmReturned is not None:\n",
    "          fig, ax = plt.subplots(figsize=(7, 7))\n",
    "          ConfusionMatrixDisplay(cmReturned).plot(ax=ax,cmap='Blues', xticks_rotation='vertical', values_format='d')\n",
    "          plt.show()   \n",
    "      MODELS.append(tempModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38ab9fe",
   "metadata": {},
   "source": [
    "## Storing the loss,training accuracy,validation accuracy in a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0396745b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the lists from the file\n",
    "with open(saveFilePath, 'wb') as f:\n",
    "    data = {\n",
    "        'hyper_parameters': hyper_parameters,\n",
    "        'train_acc_hyper_paramaters': train_acc_hyper_paramaters,\n",
    "        'train_acc_valid_hyper_paramaters': train_acc_valid_hyper_paramaters,\n",
    "        'train_loss_hyper_paramaters': train_loss_hyper_paramaters,\n",
    "        'train_acc_hyper_paramaters_step': train_acc_hyper_paramaters_step,\n",
    "        'train_acc_valid_hyper_paramaters_step': train_acc_valid_hyper_paramaters_step,\n",
    "        'train_loss_hyper_paramaters_step': train_loss_hyper_paramaters_step,\n",
    "        'test_loss': test_loss,\n",
    "    }\n",
    "    pickle.dump(data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc9dae9",
   "metadata": {},
   "source": [
    "## Loading the loss,training accuracy,validation accuracy in a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199a74a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the lists from the file\n",
    "saveFilePath=\"C:/Users/vshar/Documents/hyperparameters_ASL_Final_Hyper.pkl\"\n",
    "saveModelPath=\"C:/Users/vshar/Documents\"\n",
    "with open(saveFilePath, 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "    hyper_parameters = data['hyper_parameters']\n",
    "    train_acc_hyper_paramaters = data['train_acc_hyper_paramaters']\n",
    "    train_acc_valid_hyper_paramaters = data['train_acc_valid_hyper_paramaters']\n",
    "    train_loss_hyper_paramaters = data['train_loss_hyper_paramaters']\n",
    "    train_acc_hyper_paramaters_step = data['train_acc_hyper_paramaters_step']\n",
    "    train_acc_valid_hyper_paramaters_step = data['train_acc_valid_hyper_paramaters_step']\n",
    "    train_loss_hyper_paramaters_step = data['train_loss_hyper_paramaters_step']\n",
    "    test_loss=data['test_loss']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751bd0ce",
   "metadata": {},
   "source": [
    "## Saving the Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925b5917",
   "metadata": {},
   "outputs": [],
   "source": [
    "i=1\n",
    "saveModelPath=\"C:/Users/vshar/Documents/TASK1\"\n",
    "for i in range(len(MODELS)):\n",
    "    torch.save(MODELS[i], saveModelPath+\"/COMP6321_ResNet_Task1_CancerDataset_Model_Final_HyperParamaterTuning\"+str(i)+\".pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c4d365",
   "metadata": {},
   "source": [
    "## Setting the font size and creaing list of epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc41e358",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=10  \"\"\"Creating a list of epochs used for generating the plots\"\"\" \n",
    "num_epochs = [(i+1) for i in range(epochs)]\n",
    "plt.rcParams.update({'font.size': 15})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549480f8",
   "metadata": {},
   "source": [
    "## Accuracy Plots(without smoothening)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bffe891",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy plot')\n",
    "\n",
    "\n",
    "for i in range(len(hyper_parameters)):\n",
    "    plt.plot(num_epochs,train_acc_hyper_paramaters[i],label=hyper_parameters[i][0:2])\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad16c9d7",
   "metadata": {},
   "source": [
    "## Accuracy Plots(without smoothening)(Selected Learning rates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1da4c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy plot')\n",
    "\n",
    "for i in range(0, len(hyper_parameters), 6):\n",
    "    for j in range(3):\n",
    "        if i + j < len(hyper_parameters):\n",
    "            plt.plot(num_epochs, train_acc_hyper_paramaters[i + j], label=hyper_parameters[i + j][0:2])\n",
    "    plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "plt.show()  # Display each set of 3 plots separately\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10c3e69",
   "metadata": {},
   "source": [
    "## Accuracy Plots(without smoothening)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0abccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "num_epochs = [(i+1) for i in range(epochs)]\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy plot(with smoothening)')\n",
    "\n",
    "window_size = 3  # choose the window size for smoothing\n",
    "\n",
    "for i in range(len(hyper_parameters)):\n",
    "    accuracy = train_acc_hyper_paramaters[i]\n",
    "    smoothed_accuracy = np.convolve(accuracy, np.ones(window_size)/window_size, mode='valid')\n",
    "    plt.plot(num_epochs[-len(smoothed_accuracy):], smoothed_accuracy, label=hyper_parameters[i][0:2])\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f753d134",
   "metadata": {},
   "source": [
    "## Accuracy Plots(with smoothening)(Selected Learning rates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9a2465",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "epochs = 10\n",
    "num_epochs = [(i+1) for i in range(epochs)]\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy plot (with smoothening)')\n",
    "\n",
    "window_size = 3  # choose the window size for smoothing\n",
    "\n",
    "for i in range(0, len(hyper_parameters), 6):\n",
    "    for j in range(3):\n",
    "        if i + j < len(hyper_parameters):\n",
    "            accuracy = train_acc_hyper_paramaters[i + j]\n",
    "            smoothed_accuracy = np.convolve(accuracy, np.ones(window_size)/window_size, mode='valid')\n",
    "            plt.plot(num_epochs[-len(smoothed_accuracy):], smoothed_accuracy, label=hyper_parameters[i + j][0:2])\n",
    "    plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "plt.show()  # Display each set of 3 plots separately\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6310d29",
   "metadata": {},
   "source": [
    "## Test Accuracy and Learning Rate(Scatter plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0777737",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "plt.xlabel('Learning Rate')\n",
    "plt.ylabel('Test Accuracy %')\n",
    "plt.title('Test Accuracy plot')\n",
    "\n",
    "\n",
    "for i in range(len(hyper_parameters)):\n",
    "    #print(test_loss[i][0],test_loss[i][1])\n",
    "    plt.plot(test_loss[i][1],test_loss[i][0], marker='o', markersize=10,label=hyper_parameters[i][0:2])\n",
    "    \n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2640c6",
   "metadata": {},
   "source": [
    "## Test Accuracy and Learning Rate(Scatter plot)(Selective Learning Rates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eeefa1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.xlabel('Learning Rate')\n",
    "plt.ylabel('Test Accuracy %')\n",
    "plt.title('Test Accuracy plot')\n",
    "\n",
    "for i in range(0, len(hyper_parameters), 6):\n",
    "    for j in range(3):\n",
    "        if i + j < len(hyper_parameters):\n",
    "            plt.plot(test_loss[i + j][1], test_loss[i + j][0], marker='o', markersize=10, label=hyper_parameters[i + j][0:2])\n",
    "    plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "plt.show()  # Display each set of 3 plots separately\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e754c5",
   "metadata": {},
   "source": [
    "## Loss Plot(without smoothening)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5db924",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss plot')\n",
    "#plt.ylim(0,3)\n",
    "print()\n",
    "\n",
    "for i in range(len(hyper_parameters)):\n",
    "    #print(train_loss_hyper_paramaters[i])\n",
    "    plt.plot(train_loss_hyper_paramaters[i],label=hyper_parameters[i][0:2])\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0daa5cc",
   "metadata": {},
   "source": [
    "## Loss Plot(with smoothening)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2688afd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "num_epochs = [(i+1) for i in range(epochs)]\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss plot(with smoothening)')\n",
    "#plt.ylim(0,3)\n",
    "window_size = 3  # choose the window size for smoothing\n",
    "\n",
    "for i in range(len(hyper_parameters)):\n",
    "    losses = train_loss_hyper_paramaters[i]\n",
    "    smoothed_losses = np.convolve(losses, np.ones(window_size)/window_size, mode='valid')\n",
    "    plt.plot(num_epochs[-len(smoothed_losses):], smoothed_losses, label=hyper_parameters[i][0:2])\n",
    "    \n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b3d1c2",
   "metadata": {},
   "source": [
    "## Loss Plot with respect to steps(without smoothening)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76e2d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 10))\n",
    "plt.xlabel('step')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss plot')\n",
    "#plt.ylim(0,3)\n",
    "\n",
    "for i in range(len(hyper_parameters)):\n",
    "    plt.plot(train_loss_hyper_paramaters_step[i],label=hyper_parameters[i][0:2])\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d1081e",
   "metadata": {},
   "source": [
    "## Loss Plot with respect to steps(with smoothening)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86ff36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 10))\n",
    "plt.xlabel('steps')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss plot')\n",
    "#plt.ylim(0,3)\n",
    "window_size = 3\n",
    "\n",
    "for i in range(len(hyper_parameters)):\n",
    "    # apply moving average filter to loss values\n",
    "    smoothed_loss = np.convolve(train_loss_hyper_paramaters_step[i], np.ones(window_size)/window_size, mode='valid')\n",
    "    plt.plot(smoothed_loss, label=hyper_parameters[i][0:2])\n",
    "\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4cd70e2",
   "metadata": {},
   "source": [
    "## Training and Validation Plots with respect to steps(without smoothing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf77b919",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=8, ncols=3, figsize=(20, 10))\n",
    "fig.suptitle('Accuracy plots', fontsize=20)\n",
    "fig.tight_layout(pad=5.0)\n",
    "for i in range(len(hyper_parameters)):\n",
    "    row = i // 3\n",
    "    col = i % 3\n",
    "    axs[row, col].plot( train_acc_hyper_paramaters_step[i], label=\"Training Accuracy\")\n",
    "    axs[row, col].plot(train_acc_valid_hyper_paramaters_step[i],label=\"Validation Accuracy\")\n",
    "    axs[row, col].set_xlabel('Steps')\n",
    "    axs[row, col].set_ylabel('Accuracy')\n",
    "    axs[row, col].set_title('Accuracy plot for ' + \" \".join(hyper_parameters[i][0:2]))\n",
    "    axs[row, col].legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17252b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=12, ncols=2, figsize=(20, 20))  # Increased figsize\n",
    "fig.suptitle('Accuracy plots', fontsize=20)\n",
    "\n",
    "for i in range(len(hyper_parameters)):\n",
    "    row = i // 2\n",
    "    col = i % 2\n",
    "    axs[row, col].plot(train_acc_hyper_paramaters_step[i], label=\"Training Accuracy\")\n",
    "    axs[row, col].plot(train_acc_valid_hyper_paramaters_step[i], label=\"Validation Accuracy\")\n",
    "    axs[row, col].set_xlabel('Steps')\n",
    "    axs[row, col].set_ylabel('Accuracy')\n",
    "    axs[row, col].set_title('Accuracy plot for ' + \" \".join(hyper_parameters[i][0:2]))\n",
    "    axs[row, col].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(top=0.92)  # Adjust the space for the title\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206d5162",
   "metadata": {},
   "source": [
    "## T-SNE Plot (Learning Rate=0.0001 and Batch size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7fcb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "temporaryModel=MODELS[8]\n",
    "\n",
    "  \n",
    "  # Get embeddings for the training data\n",
    "embeddings = []\n",
    "labels = []\n",
    "with torch.no_grad():\n",
    "    for data in train_loader:\n",
    "        images, targets = data[0].to(device), data[1].to(device)\n",
    "        outputs = temporaryModel(images)\n",
    "        embeddings.append(outputs.cpu().numpy())\n",
    "        labels.append(targets.cpu().numpy())\n",
    "embeddings = np.concatenate(embeddings)\n",
    "labels = np.concatenate(labels)\n",
    "\n",
    "  # Apply t-SNE for visualization\n",
    "tsne = TSNE(n_components=2, perplexity=30, random_state=0)\n",
    "embeddings_tsne = tsne.fit_transform(embeddings)\n",
    "\n",
    "  # Plot t-SNE embeddings\n",
    "plt.figure(figsize=(30,15))\n",
    "\n",
    "for i in range(len(labels_map)):\n",
    "    plt.scatter(embeddings_tsne[labels==i,0], embeddings_tsne[labels==i,1], label=f'Class {i}')\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "plt.title('t-SNE Embeddings for Training Data')\n",
    "plt.xlabel(\"t-SNE component 1\")\n",
    "plt.ylabel(\"t-SNE component 2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53108777",
   "metadata": {},
   "source": [
    "## T-SNE Plot (Learning Rate=0.005 and Batch size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877660ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "temporaryModel=MODELS[15]\n",
    "# Get embeddings for the training data\n",
    "embeddings = []\n",
    "labels = []\n",
    "with torch.no_grad():\n",
    "    for data in train_loader:\n",
    "        images, targets = data[0].to(device), data[1].to(device)\n",
    "        outputs = temporaryModel(images)\n",
    "        embeddings.append(outputs.cpu().numpy())\n",
    "        labels.append(targets.cpu().numpy())\n",
    "embeddings = np.concatenate(embeddings)\n",
    "labels = np.concatenate(labels)\n",
    "\n",
    "  # Apply t-SNE for visualization\n",
    "tsne = TSNE(n_components=2, perplexity=30, random_state=0)\n",
    "embeddings_tsne = tsne.fit_transform(embeddings)\n",
    "\n",
    "  # Plot t-SNE embeddings\n",
    "plt.figure(figsize=(30,15))\n",
    "\n",
    "for i in range(len(labels_map)):\n",
    "    plt.scatter(embeddings_tsne[labels==i,0], embeddings_tsne[labels==i,1], label=f'Class {i}')\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "plt.title('t-SNE Embeddings for Training Data')\n",
    "plt.xlabel(\"t-SNE component 1\")\n",
    "plt.ylabel(\"t-SNE component 2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa0d81c",
   "metadata": {},
   "source": [
    "## Model FLOPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39c246d",
   "metadata": {},
   "outputs": [],
   "source": [
    "TempTransformer = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.RandomRotation(20),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(0.1,0.1,0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.554, 0.450, 0.343],[0.231, 0.241, 0.241]),\n",
    "  ])\n",
    "\n",
    "imageInput = Image.open('C:/Users/vshar/Documents/ASL_Final/Q/10.jpg')\n",
    "input_data = TempTransformer(imageInput)\n",
    "\n",
    "input_data=input_data.view(1,3,224,224)\n",
    "print(input_data.size())\n",
    "r18net = models.resnet18()\n",
    "flops, params = profile(r18net, inputs=(input_data, ))\n",
    "print(f\"FLOPS: {flops / 1e9:.2f} billion\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
